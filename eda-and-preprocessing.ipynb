{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Sight Dataset and the Coursera Review Dataset and Preprocessing/Cleaning them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sight_df = pd.read_csv(\n",
    "    \"data/sight_dataset.csv\",\n",
    "    delimiter=\",\",               # Specify delimiter\n",
    "    quotechar='\"',               # Handle embedded quotes\n",
    "    escapechar=\"\\\\\",             # Escape special characters\n",
    "    on_bad_lines=\"skip\",         # Skip problematic lines\n",
    "    engine=\"python\"              # Use the Python parser for flexibility\n",
    ")\n",
    "\n",
    "# Upon analysis, we discovered that this dataset can be used to test since it is unlabelled.\n",
    "#sight_df.head()\n",
    "\n",
    "coursera_df = pd.read_csv(\"data/reviews_by_course.csv\")\n",
    "coursera_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting the data types and checking if there are any missing values\n",
    "print(coursera_df.info())\n",
    "print(coursera_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since only 3 missing values, dropping them\n",
    "coursera_df = coursera_df.dropna(subset=[\"Review\"])\n",
    "print(coursera_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking label distribution\n",
    "print(coursera_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''At this point, given that the dataset is skewed with a lot more rows labelled 5 than not, given the choice of undersampling label 5, oversampling the lower labels 1-4,\n",
    "and using class weights, we decided to go with class weights to avoid artificially adding data (fake reviews) to the dataset. \n",
    "'''\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "#Defining the classes and their freq\n",
    "\n",
    "classes = sorted(coursera_df[\"Label\"].unique())\n",
    "print(classes)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight = \"balanced\",\n",
    "    classes = classes,\n",
    "    y=coursera_df[\"Label\"]\n",
    ")\n",
    "\n",
    "#Converting to dictionary for easy ref\n",
    "class_weights_dict = {classes[i]: class_weights[i] for i in range(len(classes))}\n",
    "print(class_weights_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon visual inspection of the dataset, we noticed some issues:\n",
    "\n",
    "1. Non-English Reviews: Some of the reviews are in spanish. In BERT is pre-trained on English text, this will cause problems and affect fine-tuning quality.\n",
    "2. Gibberish and Encoding issues: We found that some of the reviews had plain gibberish text. eg: Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ Ð¸ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ð¾. We will either correct the encoding errors or drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install langdetect and unidecode\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "def detect_language(text):\n",
    "    try: \n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return \"unknown\"\n",
    "    \n",
    "#Detecting each review language\n",
    "coursera_df[\"Language\"] = coursera_df[\"Review\"].apply(detect_language)\n",
    "\n",
    "#filtering only English reviews\n",
    "english_reviews_df = coursera_df[coursera_df[\"Language\"]==\"en\"]\n",
    "\n",
    "print(english_reviews_df[\"Language\"].value_counts())\n",
    "english_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing giberrish\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "english_reviews_df[\"cleaned_review\"] = english_reviews_df[\"Review\"].apply(unidecode)\n",
    "\n",
    "print(english_reviews_df[\"cleaned_review\"].head())\n",
    "print(english_reviews_df.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking whether original df had more langs\n",
    "\n",
    "print(coursera_df[\"Language\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving new csv\n",
    "english_reviews_df.to_csv(\"data/coursera_english_reviews.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we only have English reviews, time to apply standard text preprocessing such as conversion to lowercase, punctuation removal, special character removal, and extra whitespace removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Applying cleaning to the Review column\n",
    "english_reviews_df[\"cleaned_review\"] = english_reviews_df[\"Review\"].apply(clean_text)\n",
    "\n",
    "# Verifying the changes\n",
    "print(english_reviews_df[[\"Review\", \"cleaned_review\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(english_reviews_df, test_size=0.2, stratify=english_reviews_df[\"Label\"], random_state=37)\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing using BERT from Hugging Face Transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing dataset to determine max_length for BERT tokenization\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "review_lengths = english_reviews_df[\"cleaned_review\"].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "print(review_lengths.describe())  # Check mean, median, and max token length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(english_reviews_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying GPU usage\n",
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load a tokenizer to test\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(\"Transformers library is functional!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given that token lenght rarely even touches 128, we will keep max_lenght as 128\n",
    "import torch\n",
    "def tokenize_data(data):\n",
    "    return tokenizer(\n",
    "        list(data[\"cleaned_review\"]),\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_data(train_data)\n",
    "val_encodings = tokenize_data(val_data)\n",
    "\n",
    "print(\"Tokenization has been completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
